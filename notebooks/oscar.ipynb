{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from scripts.scrape_oldlistings import scrape_postcodes, scrape_postcodes_from_file, get_oldlisting_data, get_remaining_suburbs, get_remaining_oldlisting_data, convert_csv_to_parquet\n",
    "from scripts.preprocess_oldlistings import preprocess_olist, create_forecast_template, split_by_gcc, split_domain_by_gcc\n",
    "# from scripts.preproccessing import combine_SA2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datascrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>suburb</th>\n",
       "      <th>postcode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abbotsford</td>\n",
       "      <td>3067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aberfeldie</td>\n",
       "      <td>3040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aberfeldy</td>\n",
       "      <td>3825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Acheron</td>\n",
       "      <td>3714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Addington</td>\n",
       "      <td>3352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>Yeodene</td>\n",
       "      <td>3249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>Yinnar</td>\n",
       "      <td>3869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700</th>\n",
       "      <td>Youanmite</td>\n",
       "      <td>3646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1701</th>\n",
       "      <td>Yundool</td>\n",
       "      <td>3727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1702</th>\n",
       "      <td>Yuroke</td>\n",
       "      <td>3063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1703 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          suburb postcode\n",
       "0     Abbotsford     3067\n",
       "1     Aberfeldie     3040\n",
       "2      Aberfeldy     3825\n",
       "3        Acheron     3714\n",
       "4      Addington     3352\n",
       "...          ...      ...\n",
       "1698     Yeodene     3249\n",
       "1699      Yinnar     3869\n",
       "1700   Youanmite     3646\n",
       "1701     Yundool     3727\n",
       "1702      Yuroke     3063\n",
       "\n",
       "[1703 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scrape_postcodes_from_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data for Abbotsford, 3067. Suburb 0 of 1703\n",
      "num pages: 96\n",
      "Getting data for Aberfeldie, 3040. Suburb 1 of 1703\n",
      "num pages: 11\n",
      "Getting data for Aberfeldy, 3825. Suburb 2 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Aberfeldy/3825/rent/. Continuing with other URLs.\n",
      "Getting data for Acheron, 3714. Suburb 3 of 1703\n",
      "num pages: 1\n",
      "Getting data for Addington, 3352. Suburb 4 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Addington/3352/rent/. Continuing with other URLs.\n",
      "Getting data for Agnes, 3962. Suburb 5 of 1703\n",
      "num pages: 1\n",
      "Getting data for Aireys+Inlet, 3231. Suburb 6 of 1703\n",
      "num pages: 5\n",
      "Getting data for Airport+West, 3042. Suburb 7 of 1703\n",
      "num pages: 45\n",
      "Getting data for Albanvale, 3021. Suburb 8 of 1703\n",
      "num pages: 12\n",
      "Getting data for Albert+Park, 3206. Suburb 9 of 1703\n",
      "num pages: 47\n",
      "Getting data for Alberton, 3971. Suburb 10 of 1703\n",
      "num pages: 1\n",
      "Getting data for Albion, 3020. Suburb 11 of 1703\n",
      "num pages: 25\n",
      "Getting data for Alexandra, 3714. Suburb 12 of 1703\n",
      "num pages: 9\n",
      "Getting data for Alfredton, 3350. Suburb 13 of 1703\n",
      "num pages: 52\n",
      "Getting data for Allans+Flat, 3691. Suburb 14 of 1703\n",
      "num pages: 1\n",
      "Getting data for Allansford, 3277. Suburb 15 of 1703\n",
      "num pages: 3\n",
      "Getting data for Allendale, 3364. Suburb 16 of 1703\n",
      "num pages: 1\n",
      "Getting data for Allestree, 3305. Suburb 17 of 1703\n",
      "num pages: 1\n",
      "Getting data for Alma, 3465. Suburb 18 of 1703\n",
      "num pages: 1\n",
      "Getting data for Almurta, 3979. Suburb 19 of 1703\n",
      "num pages: 1\n",
      "Getting data for Alphington, 3078. Suburb 20 of 1703\n",
      "num pages: 29\n",
      "Getting data for Altona, 3018. Suburb 21 of 1703\n",
      "num pages: 83\n",
      "Getting data for Altona+Meadows, 3028. Suburb 22 of 1703\n",
      "num pages: 78\n",
      "Getting data for Altona+North, 3025. Suburb 23 of 1703\n",
      "num pages: 63\n",
      "Getting data for Alvie, 3249. Suburb 24 of 1703\n",
      "num pages: 1\n",
      "Getting data for Amphitheatre, 3377. Suburb 25 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Amphitheatre/3377/rent/. Continuing with other URLs.\n",
      "Getting data for Anakie, 3221. Suburb 26 of 1703\n",
      "num pages: 1\n",
      "Getting data for Ancona, 3715. Suburb 27 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Ancona/3715/rent/. Continuing with other URLs.\n",
      "Getting data for Anglesea, 3230. Suburb 28 of 1703\n",
      "num pages: 17\n",
      "Getting data for Antwerp, 3414. Suburb 29 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Antwerp/3414/rent/. Continuing with other URLs.\n",
      "Getting data for Apollo+Bay, 3615607. Suburb 30 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Apollo+Bay/3615607/rent/. Continuing with other URLs.\n",
      "Getting data for Appin+Park, 3677. Suburb 31 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Appin+Park/3677/rent/. Continuing with other URLs.\n",
      "Getting data for Apsley, 3319. Suburb 32 of 1703\n",
      "num pages: 1\n",
      "Getting data for Ararat, 3377. Suburb 33 of 1703\n",
      "num pages: 40\n",
      "Getting data for Arcadia, 3631. Suburb 34 of 1703\n",
      "num pages: 1\n",
      "Getting data for Archies+Creek, 3995. Suburb 35 of 1703\n",
      "num pages: 1\n",
      "Getting data for Ardeer, 3022. Suburb 36 of 1703\n",
      "num pages: 16\n",
      "Getting data for Ardmona, 3629. Suburb 37 of 1703\n",
      "num pages: 1\n",
      "Getting data for Areegra, 3480. Suburb 38 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Areegra/3480/rent/. Continuing with other URLs.\n",
      "Getting data for Armadale, 3143. Suburb 39 of 1703\n",
      "num pages: 80\n",
      "Getting data for Armadale+North, 3143. Suburb 40 of 1703\n",
      "num pages: 2\n",
      "Getting data for Arnold, 3551. Suburb 41 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Arnold/3551/rent/. Continuing with other URLs.\n",
      "Getting data for Arthurs+Creek, 3099. Suburb 42 of 1703\n",
      "num pages: 1\n",
      "Getting data for Arthurs+Seat, 3936. Suburb 43 of 1703\n",
      "num pages: 1\n",
      "Getting data for Ascot, 3364. Suburb 44 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Ascot/3364/rent/. Continuing with other URLs.\n",
      "Getting data for Ascot+Vale, 3032. Suburb 45 of 1703\n",
      "num pages: 85\n",
      "Getting data for Ashbourne, 3442. Suburb 46 of 1703\n",
      "num pages: 1\n",
      "Getting data for Ashburton, 3147. Suburb 47 of 1703\n",
      "num pages: 27\n",
      "Getting data for Ashwood, 3147. Suburb 48 of 1703\n",
      "num pages: 34\n",
      "Getting data for Aspendale, 3195. Suburb 49 of 1703\n",
      "num pages: 28\n",
      "Getting data for Aspendale+Gardens, 3195. Suburb 50 of 1703\n",
      "num pages: 9\n",
      "Getting data for Athlone, 3818. Suburb 51 of 1703\n",
      "num pages: 1\n",
      "Getting data for Attwood, 3049. Suburb 52 of 1703\n",
      "num pages: 4\n",
      "Getting data for Avenel, 3664. Suburb 53 of 1703\n",
      "num pages: 4\n",
      "Getting data for Avoca, 3467. Suburb 54 of 1703\n",
      "num pages: 3\n",
      "Getting data for Avondale+Heights, 3034. Suburb 55 of 1703\n",
      "num pages: 43\n",
      "Getting data for Avonsleigh, 3782. Suburb 56 of 1703\n",
      "num pages: 2\n",
      "Getting data for Axedale, 3551. Suburb 57 of 1703\n",
      "num pages: 1\n",
      "Getting data for Baarmutha, 3747. Suburb 58 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Baarmutha/3747/rent/. Continuing with other URLs.\n",
      "Getting data for Bacchus+Marsh, 3340. Suburb 59 of 1703\n",
      "num pages: 68\n",
      "Getting data for Baddaginnie, 3670. Suburb 60 of 1703\n",
      "num pages: 1\n",
      "Getting data for Bailieston, 3608. Suburb 61 of 1703\n",
      "num pages: 1\n",
      "Getting data for Bairnsdale, 3875. Suburb 62 of 1703\n",
      "num pages: 65\n",
      "Getting data for Bakery+Hill, 3354. Suburb 63 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Bakery+Hill/3354/rent/. Continuing with other URLs.\n",
      "Getting data for Balaclava, 3183. Suburb 64 of 1703\n",
      "num pages: 30\n",
      "Getting data for Ballan, 3342. Suburb 65 of 1703\n",
      "num pages: 7\n",
      "Getting data for Ballarat, 3350. Suburb 66 of 1703\n",
      "num pages: 140\n",
      "Getting data for Ballarat, 3353. Suburb 67 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Ballarat/3353/rent/. Continuing with other URLs.\n",
      "Getting data for Ballarat+Mc, 3354. Suburb 68 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Ballarat+Mc/3354/rent/. Continuing with other URLs.\n",
      "Getting data for Ballarat+North, 3350. Suburb 69 of 1703\n",
      "num pages: 22\n",
      "Getting data for Ballarat+West, 3350. Suburb 70 of 1703\n",
      "num pages: 1\n",
      "Getting data for Balliang, 3340. Suburb 71 of 1703\n",
      "num pages: 1\n",
      "Getting data for Balmattum, 3666. Suburb 72 of 1703\n",
      "num pages: 1\n",
      "Getting data for Balmoral, 3407. Suburb 73 of 1703\n",
      "num pages: 2\n",
      "Getting data for Balnarring, 3926. Suburb 74 of 1703\n",
      "num pages: 4\n",
      "Getting data for Balook, 3971. Suburb 75 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Balook/3971/rent/. Continuing with other URLs.\n",
      "Getting data for Balwyn, 3103. Suburb 76 of 1703\n",
      "num pages: 70\n",
      "Getting data for Balwyn+North, 3104. Suburb 77 of 1703\n",
      "num pages: 70\n",
      "Getting data for Bamawm, 3561. Suburb 78 of 1703\n",
      "num pages: 1\n",
      "Getting data for Bambra, 3241. Suburb 79 of 1703\n",
      "num pages: 1\n",
      "Getting data for Bandiana+Milpo, 3694. Suburb 80 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Bandiana+Milpo/3694/rent/. Continuing with other URLs.\n",
      "Getting data for Bangholme, 3175. Suburb 81 of 1703\n",
      "num pages: 2\n",
      "Getting data for Bannockburn, 3331. Suburb 82 of 1703\n",
      "num pages: 10\n",
      "Getting data for Banyena, 3388. Suburb 83 of 1703\n",
      "num pages: 1\n",
      "Getting data for Banyule, 3084. Suburb 84 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Banyule/3084/rent/. Continuing with other URLs.\n",
      "Getting data for Baranduda, 3691. Suburb 85 of 1703\n",
      "num pages: 7\n",
      "Getting data for Baringhup, 3463. Suburb 86 of 1703\n",
      "num pages: 1\n",
      "Getting data for Barkers+Creek, 3451. Suburb 87 of 1703\n",
      "num pages: 1\n",
      "Getting data for Barkly, 3381. Suburb 88 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Barkly/3381/rent/. Continuing with other URLs.\n",
      "Getting data for Barkstead, 3352. Suburb 89 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Barkstead/3352/rent/. Continuing with other URLs.\n",
      "Getting data for Barmah, 3639. Suburb 90 of 1703\n",
      "num pages: 1\n",
      "Getting data for Barnadown, 3557. Suburb 91 of 1703\n",
      "num pages: 1\n",
      "Getting data for Barnawartha, 3688. Suburb 92 of 1703\n",
      "num pages: 2\n",
      "Getting data for Barongarook, 3249. Suburb 93 of 1703\n",
      "num pages: 1\n",
      "Getting data for Barrabool, 3221. Suburb 94 of 1703\n",
      "num pages: 1\n",
      "Getting data for Barramunga, 3249. Suburb 95 of 1703\n",
      "num pages: 1\n",
      "Getting data for Barunah+Park, 3329. Suburb 96 of 1703\n",
      "num pages: 1\n",
      "Getting data for Barwon+Downs, 3243. Suburb 97 of 1703\n",
      "num pages: 1\n",
      "Getting data for Barwon+Heads, 3227. Suburb 98 of 1703\n",
      "num pages: 16\n",
      "Getting data for Bass, 3991. Suburb 99 of 1703\n",
      "num pages: 1\n",
      "Getting data for Batesford, 3221. Suburb 100 of 1703\n",
      "num pages: 1\n",
      "Getting data for Baxter, 3911. Suburb 101 of 1703\n",
      "num pages: 6\n",
      "Getting data for Bayindeen, 3375. Suburb 102 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Bayindeen/3375/rent/. Continuing with other URLs.\n",
      "Getting data for Bayles, 3981. Suburb 103 of 1703\n",
      "num pages: 1\n",
      "Getting data for Bayswater, 3153. Suburb 104 of 1703\n",
      "num pages: 77\n",
      "Getting data for Bayswater+North, 3153. Suburb 105 of 1703\n",
      "num pages: 28\n",
      "Getting data for Beaconsfield, 3807. Suburb 106 of 1703\n",
      "num pages: 19\n",
      "Getting data for Beaconsfield+Upper, 3808. Suburb 107 of 1703\n",
      "num pages: 3\n",
      "Getting data for Bealiba, 3475. Suburb 108 of 1703\n",
      "num pages: 1\n",
      "Getting data for Beaufort, 3373. Suburb 109 of 1703\n",
      "num pages: 4\n",
      "Getting data for Beaumaris, 3193. Suburb 110 of 1703\n",
      "num pages: 38\n",
      "Getting data for Bedford+Road, 3135. Suburb 111 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Bedford+Road/3135/rent/. Continuing with other URLs.\n",
      "Getting data for Beeac, 3251. Suburb 112 of 1703\n",
      "num pages: 1\n",
      "Getting data for Beech+Forest, 3237. Suburb 113 of 1703\n",
      "num pages: 1\n",
      "Getting data for Beechworth, 3747. Suburb 114 of 1703\n",
      "num pages: 11\n",
      "Getting data for Belgrave, 3160. Suburb 115 of 1703\n",
      "num pages: 8\n",
      "Getting data for Bell+Park, 3215. Suburb 116 of 1703\n",
      "num pages: 24\n",
      "Getting data for Bell+Post+Hill, 3215. Suburb 117 of 1703\n",
      "num pages: 17\n",
      "Getting data for Bellarine, 3221. Suburb 118 of 1703\n",
      "num pages: 1\n",
      "Getting data for Bellbrae, 3228. Suburb 119 of 1703\n",
      "num pages: 1\n",
      "Getting data for Bellbridge, 3691. Suburb 120 of 1703\n",
      "num pages: 1\n",
      "Getting data for Belmont, 3216. Suburb 121 of 1703\n",
      "num pages: 83\n",
      "Getting data for Belvedere+Park, 3198. Suburb 122 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Belvedere+Park/3198/rent/. Continuing with other URLs.\n",
      "Getting data for Bemm+River, 3889. Suburb 123 of 1703\n",
      "num pages: 1\n",
      "Getting data for Bena, 3946. Suburb 124 of 1703\n",
      "num pages: 1\n",
      "Getting data for Benalla, 3671. Suburb 125 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Benalla/3671/rent/. Continuing with other URLs.\n",
      "Getting data for Benalla, 3672. Suburb 126 of 1703\n",
      "num pages: 64\n",
      "Getting data for Benalla, 3673. Suburb 127 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Benalla/3673/rent/. Continuing with other URLs.\n",
      "Getting data for Benambra, 3900. Suburb 128 of 1703\n",
      "num pages: 1\n",
      "Getting data for Benarch, 3630. Suburb 129 of 1703\n",
      "num pages: 1\n",
      "Getting data for Benayeo, 3319. Suburb 130 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Benayeo/3319/rent/. Continuing with other URLs.\n",
      "Getting data for Bend+Of+Islands, 3097. Suburb 131 of 1703\n",
      "num pages: 1\n",
      "Getting data for Bendigo, 3550. Suburb 132 of 1703\n",
      "num pages: 106\n",
      "Getting data for Bendigo, 3552. Suburb 133 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Bendigo/3552/rent/. Continuing with other URLs.\n",
      "Getting data for Bendigo+Dc, 3554. Suburb 134 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Bendigo+Dc/3554/rent/. Continuing with other URLs.\n",
      "Getting data for Bendoc, 3888. Suburb 135 of 1703\n",
      "num pages: 1\n",
      "Getting data for Bengworden, 3875. Suburb 136 of 1703\n",
      "num pages: 1\n",
      "Getting data for Bennettswood, 3125. Suburb 137 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Bennettswood/3125/rent/. Continuing with other URLs.\n",
      "Getting data for Bennison, 3960. Suburb 138 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Bennison/3960/rent/. Continuing with other URLs.\n",
      "Getting data for Bentleigh, 3204. Suburb 139 of 1703\n",
      "num pages: 90\n",
      "Getting data for Bentleigh+East, 3165. Suburb 140 of 1703\n",
      "num pages: 123\n",
      "Getting data for Beremboke, 3342. Suburb 141 of 1703\n",
      "num pages: 1\n",
      "Getting data for Berrimal, 3518. Suburb 142 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Berrimal/3518/rent/. Continuing with other URLs.\n",
      "Getting data for Berringa, 3351. Suburb 143 of 1703\n",
      "num pages: 1\n",
      "Getting data for Berringama, 3691. Suburb 144 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Berringama/3691/rent/. Continuing with other URLs.\n",
      "Getting data for Berriwillock, 3531. Suburb 145 of 1703\n",
      "num pages: 1\n",
      "Getting data for Berrybank, 3323. Suburb 146 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Berrybank/3323/rent/. Continuing with other URLs.\n",
      "Getting data for Berrys+Creek, 3953. Suburb 147 of 1703\n",
      "num pages: 1\n",
      "Getting data for Berwick, 3806. Suburb 148 of 1703\n",
      "num pages: 186\n",
      "Getting data for Bet+Bet, 3472. Suburb 149 of 1703\n",
      "num pages: 1\n",
      "Getting data for Bethanga, 3691. Suburb 150 of 1703\n",
      "num pages: 1\n",
      "Getting data for Betley, 3472. Suburb 151 of 1703\n",
      "num pages: 1\n",
      "Getting data for Beulah, 3395. Suburb 152 of 1703\n",
      "num pages: 1\n",
      "Getting data for Beverford, 3590. Suburb 153 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Beverford/3590/rent/. Continuing with other URLs.\n",
      "Getting data for Beveridge, 3753. Suburb 154 of 1703\n",
      "num pages: 13\n",
      "Getting data for Biggara, 3707. Suburb 155 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Biggara/3707/rent/. Continuing with other URLs.\n",
      "Getting data for Binginwarri, 3966. Suburb 156 of 1703\n",
      "num pages: 1\n",
      "Getting data for Birchip, 3483. Suburb 157 of 1703\n",
      "num pages: 1\n",
      "Getting data for Birregurra, 3242. Suburb 158 of 1703\n",
      "num pages: 2\n",
      "Getting data for Bittern, 3918. Suburb 159 of 1703\n",
      "num pages: 8\n",
      "Getting data for Black+Hill, 3350. Suburb 160 of 1703\n",
      "num pages: 11\n",
      "Getting data for Black+Rock, 3193. Suburb 161 of 1703\n",
      "num pages: 24\n",
      "Getting data for Blackburn, 3130. Suburb 162 of 1703\n",
      "num pages: 71\n",
      "Getting data for Blackburn+North, 3130. Suburb 163 of 1703\n",
      "num pages: 25\n",
      "Getting data for Blackburn+South, 3130. Suburb 164 of 1703\n",
      "num pages: 30\n",
      "Getting data for Blackwood, 3458. Suburb 165 of 1703\n",
      "num pages: 2\n",
      "Getting data for Blairgowrie, 3942. Suburb 166 of 1703\n",
      "num pages: 17\n",
      "Getting data for Blakeville, 3342. Suburb 167 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Blakeville/3342/rent/. Continuing with other URLs.\n",
      "Getting data for Blampied, 3364. Suburb 168 of 1703\n",
      "num pages: 1\n",
      "Getting data for Blind+Bight, 3980. Suburb 169 of 1703\n",
      "num pages: 2\n",
      "Getting data for Blowhard, 3352. Suburb 170 of 1703\n",
      "num pages: 1\n",
      "Getting data for Bogong, 3699. Suburb 171 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Bogong/3699/rent/. Continuing with other URLs.\n",
      "Getting data for Boho+South, 3669. Suburb 172 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Boho+South/3669/rent/. Continuing with other URLs.\n",
      "Getting data for Boisdale, 3860. Suburb 173 of 1703\n",
      "num pages: 1\n",
      "Getting data for Bolinda, 3432. Suburb 174 of 1703\n",
      "num pages: 1\n",
      "Getting data for Bolton, 3546. Suburb 175 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Bolton/3546/rent/. Continuing with other URLs.\n",
      "Getting data for Bolwarra, 3305. Suburb 176 of 1703\n",
      "num pages: 1\n",
      "Getting data for Bolwarrah, 3352. Suburb 177 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Bolwarrah/3352/rent/. Continuing with other URLs.\n",
      "Getting data for Bonang, 3888. Suburb 178 of 1703\n",
      "num pages: 1\n",
      "Getting data for Bonbeach, 3196. Suburb 179 of 1703\n",
      "num pages: 42\n",
      "Getting data for Bonegilla, 3691. Suburb 180 of 1703\n",
      "num pages: 1\n",
      "Getting data for Bonnie+Doon, 3720. Suburb 181 of 1703\n",
      "num pages: 1\n",
      "Getting data for Boolarra, 3870. Suburb 182 of 1703\n",
      "num pages: 2\n",
      "Getting data for Boorcan, 3265. Suburb 183 of 1703\n",
      "num pages: 1\n",
      "Getting data for Boorhaman, 3678. Suburb 184 of 1703\n",
      "num pages: 1\n",
      "Getting data for Boorhaman+North, 3685. Suburb 185 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Boorhaman+North/3685/rent/. Continuing with other URLs.\n",
      "Getting data for Boort, 3537. Suburb 186 of 1703\n",
      "num pages: 1\n",
      "Getting data for Boronia, 3155. Suburb 187 of 1703\n",
      "num pages: 119\n",
      "Getting data for Borung, 3518. Suburb 188 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Borung/3518/rent/. Continuing with other URLs.\n",
      "Getting data for Botanic+Ridge, 3977. Suburb 189 of 1703\n",
      "num pages: 9\n",
      "Getting data for Boundary+Bend, 3599. Suburb 190 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Boundary+Bend/3599/rent/. Continuing with other URLs.\n",
      "Getting data for Bowmans+Forest, 3735. Suburb 191 of 1703\n",
      "num pages: 1\n",
      "Getting data for Box+Hill, 3128. Suburb 192 of 1703\n",
      "num pages: 138\n",
      "Getting data for Box+Hill+North, 3129. Suburb 193 of 1703\n",
      "num pages: 57\n",
      "Getting data for Box+Hill+South, 3128. Suburb 194 of 1703\n",
      "num pages: 38\n",
      "Getting data for Boxwood, 3725. Suburb 195 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Boxwood/3725/rent/. Continuing with other URLs.\n",
      "Getting data for Braeside, 3195. Suburb 196 of 1703\n",
      "num pages: 4\n",
      "Getting data for Branditt, 3630. Suburb 197 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Branditt/3630/rent/. Continuing with other URLs.\n",
      "Getting data for Branxholme, 3302. Suburb 198 of 1703\n",
      "num pages: 1\n",
      "Getting data for Braybrook, 3019. Suburb 199 of 1703\n",
      "num pages: 42\n",
      "Getting data for Breakwater, 3219. Suburb 200 of 1703\n",
      "num pages: 6\n",
      "Getting data for Breamlea, 3227. Suburb 201 of 1703\n",
      "num pages: 1\n",
      "Getting data for Brentford+Square, 3131. Suburb 202 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Brentford+Square/3131/rent/. Continuing with other URLs.\n",
      "Getting data for Briagolong, 3860. Suburb 203 of 1703\n",
      "num pages: 2\n",
      "Getting data for Briar+Hill, 3088. Suburb 204 of 1703\n",
      "num pages: 10\n",
      "Getting data for Bridgewater+On+Loddon, 3516. Suburb 205 of 1703\n",
      "num pages: 1\n",
      "Getting data for Bright, 3741. Suburb 206 of 1703\n",
      "num pages: 12\n",
      "Getting data for Brighton, 3186. Suburb 207 of 1703\n",
      "num pages: 130\n",
      "Getting data for Brighton+East, 3187. Suburb 208 of 1703\n",
      "num pages: 60\n",
      "Getting data for Brighton+Road, 3184. Suburb 209 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Brighton+Road/3184/rent/. Continuing with other URLs.\n",
      "Getting data for Brim, 3391. Suburb 210 of 1703\n",
      "num pages: 1\n",
      "Getting data for Broadford, 3658. Suburb 211 of 1703\n",
      "num pages: 14\n",
      "Getting data for Broadmeadows, 3047. Suburb 212 of 1703\n",
      "num pages: 63\n",
      "Getting data for Broadwater, 3301. Suburb 213 of 1703\n",
      "num pages: 1\n",
      "Getting data for Brookfield, 3338. Suburb 214 of 1703\n",
      "num pages: 41\n",
      "Getting data for Brooklyn, 3012. Suburb 215 of 1703\n",
      "num pages: 17\n",
      "Getting data for Broomfield, 3364. Suburb 216 of 1703\n",
      "num pages: 1\n",
      "Getting data for Brown+Hill, 3350. Suburb 217 of 1703\n",
      "num pages: 14\n",
      "Getting data for Browns+Plains, 3685. Suburb 218 of 1703\n",
      "num pages: 2\n",
      "Getting data for Brunswick, 3056. Suburb 219 of 1703\n",
      "num pages: 238\n",
      "Getting data for Brunswick+East, 3057. Suburb 220 of 1703\n",
      "num pages: 121\n",
      "Getting data for Brunswick+West, 3055. Suburb 221 of 1703\n",
      "num pages: 112\n",
      "Getting data for Bruthen, 3885. Suburb 222 of 1703\n",
      "num pages: 2\n",
      "Getting data for Buangor, 3375. Suburb 223 of 1703\n",
      "num pages: 1\n",
      "Getting data for Buchan, 3885. Suburb 224 of 1703\n",
      "num pages: 1\n",
      "Getting data for Buckley, 3240. Suburb 225 of 1703\n",
      "num pages: 1\n",
      "Getting data for Buffalo, 3958. Suburb 226 of 1703\n",
      "num pages: 1\n",
      "Getting data for Buffalo+River, 3737. Suburb 227 of 1703\n",
      "num pages: 1\n",
      "Getting data for Bulla, 3428. Suburb 228 of 1703\n",
      "num pages: 1\n",
      "Getting data for Bullarook, 3352. Suburb 229 of 1703\n",
      "num pages: 1\n",
      "Getting data for Bullarto, 3461. Suburb 230 of 1703\n",
      "num pages: 1\n",
      "Getting data for Bulleen, 3105. Suburb 231 of 1703\n",
      "num pages: 38\n",
      "Getting data for Bullengarook, 3437. Suburb 232 of 1703\n",
      "num pages: 1\n",
      "Getting data for Buln+Buln, 3821. Suburb 233 of 1703\n",
      "num pages: 1\n",
      "Getting data for Bunbartha, 3634. Suburb 234 of 1703\n",
      "num pages: 1\n",
      "Getting data for Bundalaguah, 3851. Suburb 235 of 1703\n",
      "num pages: 1\n",
      "Getting data for Bundalong, 3730. Suburb 236 of 1703\n",
      "num pages: 2\n",
      "Getting data for Bundoora, 3083. Suburb 237 of 1703\n",
      "num pages: 153\n",
      "Getting data for Bungador, 3260. Suburb 238 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Bungador/3260/rent/. Continuing with other URLs.\n",
      "Getting data for Bungaree, 3352. Suburb 239 of 1703\n",
      "num pages: 1\n",
      "Getting data for Bungeet, 3726. Suburb 240 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Bungeet/3726/rent/. Continuing with other URLs.\n",
      "Getting data for Buninyong, 3357. Suburb 241 of 1703\n",
      "num pages: 9\n",
      "Getting data for Bunyip, 3815. Suburb 242 of 1703\n",
      "num pages: 7\n",
      "Getting data for Burnley, 3121. Suburb 243 of 1703\n",
      "num pages: 4\n",
      "Getting data for Burnside, 3023. Suburb 244 of 1703\n",
      "num pages: 10\n",
      "Getting data for Burrowye, 3709. Suburb 245 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Burrowye/3709/rent/. Continuing with other URLs.\n",
      "Getting data for Burrumbeet, 3352. Suburb 246 of 1703\n",
      "num pages: 1\n",
      "Getting data for Burwood, 3125. Suburb 247 of 1703\n",
      "num pages: 91\n",
      "Getting data for Burwood+East, 3151. Suburb 248 of 1703\n",
      "num pages: 45\n",
      "Getting data for Burwood+Heights, 3151. Suburb 249 of 1703\n",
      "num pages: 1\n",
      "Getting data for Bushfield, 3281. Suburb 250 of 1703\n",
      "num pages: 1\n",
      "Getting data for Butchers+Ridge, 3885. Suburb 251 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Butchers+Ridge/3885/rent/. Continuing with other URLs.\n",
      "Getting data for Buxton, 3711. Suburb 252 of 1703\n",
      "num pages: 2\n",
      "Getting data for Byaduk, 3301. Suburb 253 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Byaduk/3301/rent/. Continuing with other URLs.\n",
      "Getting data for Bylands, 3762. Suburb 254 of 1703\n",
      "num pages: 1\n",
      "Getting data for Byrneside, 3617. Suburb 255 of 1703\n",
      "num pages: 1\n",
      "Getting data for Cabarita, 3505. Suburb 256 of 1703\n",
      "num pages: 1\n",
      "Getting data for Cabbage+Tree+Creek, 3889. Suburb 257 of 1703\n",
      "num pages: 1\n",
      "Getting data for Cairnlea, 3023. Suburb 258 of 1703\n",
      "num pages: 17\n",
      "Getting data for Caldermeade, 3984. Suburb 259 of 1703\n",
      "num pages: 1\n",
      "Getting data for California+Gully, 3556. Suburb 260 of 1703\n",
      "num pages: 15\n",
      "Getting data for Calivil, 3573. Suburb 261 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Calivil/3573/rent/. Continuing with other URLs.\n",
      "Getting data for Callignee, 3844. Suburb 262 of 1703\n",
      "num pages: 1\n",
      "Getting data for Calulu, 3875. Suburb 263 of 1703\n",
      "num pages: 1\n",
      "Getting data for Camberwell, 3124. Suburb 264 of 1703\n",
      "num pages: 96\n",
      "Getting data for Camberwell+East, 3126. Suburb 265 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Camberwell+East/3126/rent/. Continuing with other URLs.\n",
      "Getting data for Camberwell+North, 3124. Suburb 266 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Camberwell+North/3124/rent/. Continuing with other URLs.\n",
      "Getting data for Camberwell+South, 3124. Suburb 267 of 1703\n",
      "num pages: 3\n",
      "Getting data for Camberwell+West, 3124. Suburb 268 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Camberwell+West/3124/rent/. Continuing with other URLs.\n",
      "Getting data for Cambrian+Hill, 3352. Suburb 269 of 1703\n",
      "num pages: 1\n",
      "Getting data for Campbellfield, 3061. Suburb 270 of 1703\n",
      "num pages: 19\n",
      "Getting data for Campbells+Bridge, 3381. Suburb 271 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Campbells+Bridge/3381/rent/. Continuing with other URLs.\n",
      "Getting data for Campbells+Creek, 3451. Suburb 272 of 1703\n",
      "num pages: 4\n",
      "Getting data for Campbells+Forest, 3556. Suburb 273 of 1703\n",
      "num pages: 1\n",
      "Getting data for Campbelltown, 3364. Suburb 274 of 1703\n",
      "num pages: 13\n",
      "Getting data for Camperdown, 3260. Suburb 275 of 1703\n",
      "num pages: 16\n",
      "Getting data for Canadian, 3350. Suburb 276 of 1703\n",
      "num pages: 19\n",
      "Getting data for Caniambo, 3630. Suburb 277 of 1703\n",
      "num pages: 1\n",
      "Getting data for Cann+River, 3890. Suburb 278 of 1703\n",
      "num pages: 1\n",
      "Getting data for Cannie, 3540. Suburb 279 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Cannie/3540/rent/. Continuing with other URLs.\n",
      "Getting data for Cannons+Creek, 3977. Suburb 280 of 1703\n",
      "num pages: 2\n",
      "Getting data for Canterbury, 3126. Suburb 281 of 1703\n",
      "num pages: 29\n",
      "Getting data for Cape+Clear, 3351. Suburb 282 of 1703\n",
      "num pages: 1\n",
      "Getting data for Cape+Paterson, 3995. Suburb 283 of 1703\n",
      "num pages: 11\n",
      "Getting data for Cape+Woolamai, 3925. Suburb 284 of 1703\n",
      "num pages: 14\n",
      "Getting data for Caramut, 3274. Suburb 285 of 1703\n",
      "num pages: 1\n",
      "Getting data for Carapook, 3312. Suburb 286 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Carapook/3312/rent/. Continuing with other URLs.\n",
      "Getting data for Cardigan, 3352. Suburb 287 of 1703\n",
      "num pages: 1\n",
      "Getting data for Cardinia, 3978. Suburb 288 of 1703\n",
      "num pages: 1\n",
      "Getting data for Cardross, 3496. Suburb 289 of 1703\n",
      "num pages: 1\n",
      "Getting data for Carisbrook, 3464. Suburb 290 of 1703\n",
      "num pages: 2\n",
      "Getting data for Carlisle+River, 3239. Suburb 291 of 1703\n",
      "num pages: 1\n",
      "Getting data for Carlsruhe, 3442. Suburb 292 of 1703\n",
      "num pages: 1\n",
      "Getting data for Carlton, 3053. Suburb 293 of 1703\n",
      "num pages: 191\n",
      "Getting data for Carlton+North, 3054. Suburb 294 of 1703\n",
      "num pages: 54\n",
      "Getting data for Carlton+South, 3053. Suburb 295 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Carlton+South/3053/rent/. Continuing with other URLs.\n",
      "Getting data for Carnegie, 3163. Suburb 296 of 1703\n",
      "num pages: 147\n",
      "Getting data for Carngham, 3351. Suburb 297 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Carngham/3351/rent/. Continuing with other URLs.\n",
      "Getting data for Caroline+Springs, 3023. Suburb 298 of 1703\n",
      "num pages: 78\n",
      "Getting data for Carrajung, 3844. Suburb 299 of 1703\n",
      "num pages: 1\n",
      "Getting data for Carrajung+South, 3874. Suburb 300 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Carrajung+South/3874/rent/. Continuing with other URLs.\n",
      "Getting data for Carranballac, 3361. Suburb 301 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Carranballac/3361/rent/. Continuing with other URLs.\n",
      "Getting data for Carrum, 3197. Suburb 302 of 1703\n",
      "num pages: 31\n",
      "Getting data for Carrum+Downs, 3201. Suburb 303 of 1703\n",
      "num pages: 91\n",
      "Getting data for Carwarp, 3494. Suburb 304 of 1703\n",
      "num pages: 1\n",
      "Getting data for Casterton, 3311. Suburb 305 of 1703\n",
      "num pages: 1\n",
      "Getting data for Castlemaine, 3450. Suburb 306 of 1703\n",
      "num pages: 23\n",
      "Getting data for Catani, 3981. Suburb 307 of 1703\n",
      "num pages: 1\n",
      "Getting data for Cathkin, 3714. Suburb 308 of 1703\n",
      "num pages: 1\n",
      "Getting data for Caulfield, 3162. Suburb 309 of 1703\n",
      "num pages: 47\n",
      "Getting data for Caulfield+East, 3145. Suburb 310 of 1703\n",
      "num pages: 13\n",
      "Getting data for Caulfield+Junction, 3161. Suburb 311 of 1703\n",
      "num pages: 1\n",
      "Getting data for Caulfield+North, 3161. Suburb 312 of 1703\n",
      "num pages: 102\n",
      "Getting data for Caulfield+South, 3162. Suburb 313 of 1703\n",
      "num pages: 59\n",
      "Getting data for Cavendish, 3314. Suburb 314 of 1703\n",
      "num pages: 1\n",
      "Getting data for Central+Park, 3145. Suburb 315 of 1703\n",
      "num pages: 1\n",
      "Getting data for Ceres, 3221. Suburb 316 of 1703\n",
      "num pages: 1\n",
      "Getting data for Chadstone, 3148. Suburb 317 of 1703\n",
      "num pages: 68\n",
      "Getting data for Chapple+Vale, 3239. Suburb 318 of 1703\n",
      "num pages: 1\n",
      "Getting data for Charleroi, 3695. Suburb 319 of 1703\n",
      "num pages: 1\n",
      "Getting data for Charlton, 3525. Suburb 320 of 1703\n",
      "num pages: 1\n",
      "Getting data for Chatsworth, 3379. Suburb 321 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Chatsworth/3379/rent/. Continuing with other URLs.\n",
      "Getting data for Chelsea, 3196. Suburb 322 of 1703\n",
      "num pages: 65\n",
      "Getting data for Chelsea+Heights, 3196. Suburb 323 of 1703\n",
      "num pages: 12\n",
      "Getting data for Cheltenham, 3192. Suburb 324 of 1703\n",
      "num pages: 113\n",
      "Getting data for Cheltenham+East, 3192. Suburb 325 of 1703\n",
      "num pages: 5\n",
      "Getting data for Chepstowe, 3351. Suburb 326 of 1703\n",
      "num pages: 1\n",
      "Getting data for Cheshunt, 3678. Suburb 327 of 1703\n",
      "num pages: 1\n",
      "Getting data for Chetwynd, 3312. Suburb 328 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Chetwynd/3312/rent/. Continuing with other URLs.\n",
      "Getting data for Chewton, 3451. Suburb 329 of 1703\n",
      "num pages: 2\n",
      "Getting data for Childers, 3824. Suburb 330 of 1703\n",
      "num pages: 1\n",
      "Getting data for Chiltern, 3683. Suburb 331 of 1703\n",
      "num pages: 3\n",
      "Getting data for Chinkapook, 3546. Suburb 332 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Chinkapook/3546/rent/. Continuing with other URLs.\n",
      "Getting data for Chirnside+Park, 3116. Suburb 333 of 1703\n",
      "num pages: 25\n",
      "Getting data for Christmas+Hills, 3775. Suburb 334 of 1703\n",
      "num pages: 1\n",
      "Getting data for Churchill, 3842. Suburb 335 of 1703\n",
      "num pages: 20\n",
      "Getting data for Clarendon, 3352. Suburb 336 of 1703\n",
      "num pages: 1\n",
      "Getting data for Clarinda, 3169. Suburb 337 of 1703\n",
      "num pages: 23\n",
      "Getting data for Clarkefield, 3430. Suburb 338 of 1703\n",
      "num pages: 1\n",
      "Getting data for Clarkes+Hill, 3352. Suburb 339 of 1703\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Clarkes+Hill/3352/rent/. Continuing with other URLs.\n",
      "Getting data for Clayton, 3168. Suburb 340 of 1703\n",
      "num pages: 220\n",
      "Getting data for Clayton+South, 3169. Suburb 341 of 1703\n",
      "Unfortunately you've been blocked\n",
      "Saving progres... please try again later\n"
     ]
    }
   ],
   "source": [
    "#get_oldlisting_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_remaining_suburbs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data for West+Melbourne, 3003. Suburb 0 of 97\n",
      "num pages: 106\n",
      "Getting data for Westmeadows, 3049. Suburb 1 of 97\n",
      "num pages: 22\n",
      "Getting data for Westmere, 3351. Suburb 2 of 97\n",
      "num pages: 1\n",
      "Getting data for Wheelers+Hill, 3150. Suburb 3 of 97\n",
      "num pages: 48\n",
      "Getting data for White+Hills, 3550. Suburb 4 of 97\n",
      "num pages: 15\n",
      "Getting data for Whitfield, 3733. Suburb 5 of 97\n",
      "num pages: 1\n",
      "Getting data for Whitlands, 3733. Suburb 6 of 97\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Whitlands/3733/rent/. Continuing with other URLs.\n",
      "Getting data for Whittington, 3219. Suburb 7 of 97\n",
      "num pages: 14\n",
      "Getting data for Whittlesea, 3757. Suburb 8 of 97\n",
      "num pages: 14\n",
      "Getting data for Whorouly, 3735. Suburb 9 of 97\n",
      "num pages: 2\n",
      "Getting data for Wickliffe, 3379. Suburb 10 of 97\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Wickliffe/3379/rent/. Continuing with other URLs.\n",
      "Getting data for Wilby, 3728. Suburb 11 of 97\n",
      "num pages: 1\n",
      "Getting data for Willaura, 3379. Suburb 12 of 97\n",
      "num pages: 1\n",
      "Getting data for Williamstown, 3016. Suburb 13 of 97\n",
      "num pages: 75\n",
      "Getting data for Willow+Grove, 3825. Suburb 14 of 97\n",
      "num pages: 2\n",
      "Getting data for Willung, 3847. Suburb 15 of 97\n",
      "num pages: 1\n",
      "Getting data for Willung+South, 3844. Suburb 16 of 97\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Willung+South/3844/rent/. Continuing with other URLs.\n",
      "Getting data for Winchelsea, 3241. Suburb 17 of 97\n",
      "num pages: 5\n",
      "Getting data for Windermere, 3352. Suburb 18 of 97\n",
      "num pages: 1\n",
      "Getting data for Windsor, 3181. Suburb 19 of 97\n",
      "num pages: 56\n",
      "Getting data for Winnap, 3304. Suburb 20 of 97\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Winnap/3304/rent/. Continuing with other URLs.\n",
      "Getting data for Winslow, 3281. Suburb 21 of 97\n",
      "num pages: 1\n",
      "Getting data for Winton, 3673. Suburb 22 of 97\n",
      "num pages: 1\n",
      "Getting data for Wiseleigh, 3885. Suburb 23 of 97\n",
      "num pages: 1\n",
      "Getting data for Wishart, 3189. Suburb 24 of 97\n",
      "num pages: 1\n",
      "Getting data for Wodonga, 3689. Suburb 25 of 97\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Wodonga/3689/rent/. Continuing with other URLs.\n",
      "Getting data for Wodonga, 3690. Suburb 26 of 97\n",
      "num pages: 156\n",
      "Getting data for Wodonga+Plaza, 3690. Suburb 27 of 97\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Wodonga+Plaza/3690/rent/. Continuing with other URLs.\n",
      "Getting data for Wollert, 3750. Suburb 28 of 97\n",
      "num pages: 72\n",
      "Getting data for Wombelano, 3401. Suburb 29 of 97\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Wombelano/3401/rent/. Continuing with other URLs.\n",
      "Getting data for Won+Wron, 3971. Suburb 30 of 97\n",
      "num pages: 1\n",
      "Getting data for Wonga+Park, 3115. Suburb 31 of 97\n",
      "num pages: 4\n",
      "Getting data for Wongarra, 3221. Suburb 32 of 97\n",
      "num pages: 1\n",
      "Getting data for Wonthaggi, 3995. Suburb 33 of 97\n",
      "num pages: 52\n",
      "Getting data for Wood+Wood, 3596. Suburb 34 of 97\n",
      "num pages: 1\n",
      "Getting data for Woodend, 3442. Suburb 35 of 97\n",
      "num pages: 14\n",
      "Getting data for Woodfield, 3715. Suburb 36 of 97\n",
      "num pages: 1\n",
      "Getting data for Woodford, 3281. Suburb 37 of 97\n",
      "num pages: 1\n",
      "Getting data for Woodleigh, 3945. Suburb 38 of 97\n",
      "num pages: 1\n",
      "Getting data for Woods+Point, 3723. Suburb 39 of 97\n",
      "num pages: 1\n",
      "Getting data for Woodside, 3874. Suburb 40 of 97\n",
      "num pages: 1\n",
      "Getting data for Woodstock, 3751. Suburb 41 of 97\n",
      "num pages: 1\n",
      "Getting data for Woodstock+West, 3463. Suburb 42 of 97\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Woodstock+West/3463/rent/. Continuing with other URLs.\n",
      "Getting data for Wool+Wool, 3249. Suburb 43 of 97\n",
      "num pages: 1\n",
      "Getting data for Woolamai, 3995. Suburb 44 of 97\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Woolamai/3995/rent/. Continuing with other URLs.\n",
      "Getting data for Woolsthorpe, 3276. Suburb 45 of 97\n",
      "num pages: 1\n",
      "Getting data for Woomelang, 3485. Suburb 46 of 97\n",
      "num pages: 1\n",
      "Getting data for Wooragee, 3747. Suburb 47 of 97\n",
      "num pages: 1\n",
      "Getting data for Woori+Yallock, 3139. Suburb 48 of 97\n",
      "num pages: 6\n",
      "Getting data for Woorinen, 3589. Suburb 49 of 97\n",
      "num pages: 1\n",
      "Getting data for Woorinen+South, 3588. Suburb 50 of 97\n",
      "num pages: 1\n",
      "Getting data for Woorndoo, 3272. Suburb 51 of 97\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Woorndoo/3272/rent/. Continuing with other URLs.\n",
      "Getting data for World+Trade+Centre, 3005. Suburb 52 of 97\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/World+Trade+Centre/3005/rent/. Continuing with other URLs.\n",
      "Getting data for World+Trade+Centre, 8005. Suburb 53 of 97\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/World+Trade+Centre/8005/rent/. Continuing with other URLs.\n",
      "Getting data for Wulgulmerang, 3885. Suburb 54 of 97\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Wulgulmerang/3885/rent/. Continuing with other URLs.\n",
      "Getting data for Wunghnu, 3635. Suburb 55 of 97\n",
      "num pages: 1\n",
      "Getting data for Wurdiboluc, 3241. Suburb 56 of 97\n",
      "num pages: 1\n",
      "Getting data for Wurruk, 3850. Suburb 57 of 97\n",
      "num pages: 4\n",
      "Getting data for Wy+Yung, 3875. Suburb 58 of 97\n",
      "num pages: 4\n",
      "Getting data for Wycheproof, 3527. Suburb 59 of 97\n",
      "num pages: 1\n",
      "Getting data for Wychitella, 3525. Suburb 60 of 97\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Wychitella/3525/rent/. Continuing with other URLs.\n",
      "Getting data for Wye+River, 3221. Suburb 61 of 97\n",
      "num pages: 1\n",
      "Getting data for Wyndham+Vale, 3024. Suburb 62 of 97\n",
      "num pages: 128\n",
      "Getting data for Wyuna, 3620. Suburb 63 of 97\n",
      "num pages: 1\n",
      "Getting data for Yaapeet, 3424. Suburb 64 of 97\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Yaapeet/3424/rent/. Continuing with other URLs.\n",
      "Getting data for Yabba+North, 3646. Suburb 65 of 97\n",
      "num pages: 1\n",
      "Getting data for Yackandandah, 3749. Suburb 66 of 97\n",
      "num pages: 4\n",
      "Getting data for Yallambie, 3085. Suburb 67 of 97\n",
      "num pages: 8\n",
      "Getting data for Yallourn+North, 3825. Suburb 68 of 97\n",
      "num pages: 3\n",
      "Getting data for Yambuk, 3285. Suburb 69 of 97\n",
      "num pages: 1\n",
      "Getting data for Yambuna, 3621. Suburb 70 of 97\n",
      "num pages: 1\n",
      "Getting data for Yan+Yean, 3755. Suburb 71 of 97\n",
      "num pages: 1\n",
      "Getting data for Yanac, 3418. Suburb 72 of 97\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Yanac/3418/rent/. Continuing with other URLs.\n",
      "Getting data for Yanakie, 3960. Suburb 73 of 97\n",
      "num pages: 1\n",
      "Getting data for Yandoit, 3461. Suburb 74 of 97\n",
      "num pages: 1\n",
      "Getting data for Yannathan, 3981. Suburb 75 of 97\n",
      "num pages: 1\n",
      "Getting data for Yarck, 3719. Suburb 76 of 97\n",
      "num pages: 1\n",
      "Getting data for Yarra+Glen, 3775. Suburb 77 of 97\n",
      "num pages: 6\n",
      "Getting data for Yarra+Junction, 3797. Suburb 78 of 97\n",
      "num pages: 6\n",
      "Getting data for Yarragon, 3823. Suburb 79 of 97\n",
      "num pages: 6\n",
      "Getting data for Yarram, 3971. Suburb 80 of 97\n",
      "num pages: 4\n",
      "Getting data for Yarrambat, 3091. Suburb 81 of 97\n",
      "num pages: 2\n",
      "Getting data for Yarraville, 3013. Suburb 82 of 97\n",
      "num pages: 89\n",
      "Getting data for Yarraville+West, 3013. Suburb 83 of 97\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Yarraville+West/3013/rent/. Continuing with other URLs.\n",
      "Getting data for Yarrawalla, 3575. Suburb 84 of 97\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Yarrawalla/3575/rent/. Continuing with other URLs.\n",
      "Getting data for Yarrawonga, 3730. Suburb 85 of 97\n",
      "num pages: 41\n",
      "Getting data for Yarroweyah, 3644. Suburb 86 of 97\n",
      "num pages: 1\n",
      "Getting data for Yarrunga, 3677. Suburb 87 of 97\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Yarrunga/3677/rent/. Continuing with other URLs.\n",
      "Getting data for Yea, 3717. Suburb 88 of 97\n",
      "num pages: 4\n",
      "Getting data for Yellingbo, 3139. Suburb 89 of 97\n",
      "num pages: 1\n",
      "Getting data for Yendon, 3352. Suburb 90 of 97\n",
      "num pages: 1\n",
      "Getting data for Yeo, 3249. Suburb 91 of 97\n",
      "num pages: 1\n",
      "Getting data for Yeodene, 3249. Suburb 92 of 97\n",
      "num pages: 1\n",
      "Getting data for Yinnar, 3869. Suburb 93 of 97\n",
      "num pages: 3\n",
      "Getting data for Youanmite, 3646. Suburb 94 of 97\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Youanmite/3646/rent/. Continuing with other URLs.\n",
      "Getting data for Yundool, 3727. Suburb 95 of 97\n",
      "Error 404: Not found https://www.oldlistings.com.au/real-estate/VIC/Yundool/3727/rent/. Continuing with other URLs.\n",
      "Getting data for Yuroke, 3063. Suburb 96 of 97\n",
      "num pages: 1\n"
     ]
    }
   ],
   "source": [
    "#get_remaining_oldlisting_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial handling of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_csv_to_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/09/26 11:10:46 WARN Utils: Your hostname, LAPTOP-UFNFC5IK resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/09/26 11:10:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/26 11:10:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"Preliminary Analysis\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/pyrou/Documents/GitHub/project-2-group-real-estate-industry-project-31/notebooks/../scripts/preproccessing.py:162: UserWarning: CRS mismatch between the CRS of left geometries and the CRS of right geometries.\n",
      "Use `to_crs()` to reproject one of the input geometries to match the CRS of the other.\n",
      "\n",
      "Left CRS: EPSG:4326\n",
      "Right CRS: EPSG:7844\n",
      "\n",
      "  gdf_joined = gpd.sjoin(gdf_points, sf, how='left', predicate='within') # join our SA2 points with all listings\n"
     ]
    }
   ],
   "source": [
    "# sdf = spark.read.parquet(\"../data/curated/properties.parquet\")\n",
    "# sdf.show()\n",
    "# pandas = sdf.toPandas()\n",
    "\n",
    "# combined = combine_SA2(pandas)\n",
    "# print(combined.head())\n",
    "split_by_gcc(spark)\n",
    "#split_domain_by_gcc(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    21039\n",
       "suburb        21039\n",
       "postcode      21039\n",
       "address       21039\n",
       "latitude      21039\n",
       "longitude     21039\n",
       "beds          20263\n",
       "baths         20240\n",
       "cars          16857\n",
       "house_type    14849\n",
       "dates         21039\n",
       "price_str     21039\n",
       "SA2_CODE21    21039\n",
       "GCC_NAME21    21039\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# sdf = spark.read.parquet(\"../data/raw/oldlisting/greater_melb_properties_unprocessed.csv\")\n",
    "df = pd.read_csv(\"../data/raw/oldlisting/rest_of_vic_properties_unprocessed.csv\")\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "JSON DECODE ERROR                                                   (0 + 1) / 1]\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------------------+--------------+--------------+----+-----+----+-------------+----------+-----------------+------------------+-------------------------+---------------------+------------------+------------------+------------+----+------------+\n",
      "|    suburb|postcode|            address|      latitude|     longitude|beds|baths|cars|property_type|SA2_CODE21|       GCC_NAME21| dist_to_education|dist_to_parks_and_gardens|dist_to_train_station|  dist_to_shopping|dist_to_healthcare|dist_to_city|date|weekly_price|\n",
      "+----------+--------+-------------------+--------------+--------------+----+-----+----+-------------+----------+-----------------+------------------+-------------------------+---------------------+------------------+------------------+------------+----+------------+\n",
      "|avonsleigh|    3782|    15 phillip road|   -37.9208807|   145.4730828| 4.0|  2.0| 2.0|        house| 212011289|Greater Melbourne|           0.86051|        4.879060000000001|              2.95269|           3.79518|          13.20147|    59.55034|2019|       520.0|\n",
      "|avonsleigh|    3782|    15 phillip road|   -37.9208807|   145.4730828| 4.0|  2.0| 2.0|        house| 212011289|Greater Melbourne|           0.86051|        4.879060000000001|              2.95269|           3.79518|          13.20147|    59.55034|2019|       520.0|\n",
      "|avonsleigh|    3782|    15 phillip road|   -37.9208807|   145.4730828| 4.0|  2.0| 2.0|        house| 212011289|Greater Melbourne|           0.86051|        4.879060000000001|              2.95269|           3.79518|          13.20147|    59.55034|2019|       520.0|\n",
      "| glen iris|    3146| 14/38 edgar street|    -37.854723|    145.045959| 1.0|  1.0| 1.0|         unit| 208041194|Greater Melbourne|            0.9589|                  0.37515|              1.56123|0.5567300000000001|0.5536599999999999|    11.51467|2024|       360.0|\n",
      "| glen iris|    3146| 14/38 edgar street|    -37.854723|    145.045959| 1.0|  1.0| 1.0|         unit| 208041194|Greater Melbourne|            0.9589|                  0.37515|              1.56123|0.5567300000000001|0.5536599999999999|    11.51467|2024|       320.0|\n",
      "| glen iris|    3146| 14/38 edgar street|    -37.854723|    145.045959| 1.0|  1.0| 1.0|         unit| 208041194|Greater Melbourne|            0.9589|                  0.37515|              1.56123|0.5567300000000001|0.5536599999999999|    11.51467|2021|       761.5|\n",
      "| glen iris|    3146| 14/38 edgar street|    -37.854723|    145.045959| 1.0|  1.0| 1.0|         unit| 208041194|Greater Melbourne|            0.9589|                  0.37515|              1.56123|0.5567300000000001|0.5536599999999999|    11.51467|2013|       253.0|\n",
      "| glen iris|    3146| 14/38 edgar street|    -37.854723|    145.045959| 1.0|  1.0| 1.0|         unit| 208041194|Greater Melbourne|            0.9589|                  0.37515|              1.56123|0.5567300000000001|0.5536599999999999|    11.51467|2008|       189.0|\n",
      "| glen iris|    3146| 14/38 edgar street|    -37.854723|    145.045959| 1.0|  1.0| 1.0|         unit| 208041194|Greater Melbourne|            0.9589|                  0.37515|              1.56123|0.5567300000000001|0.5536599999999999|    11.51467|2008|       189.0|\n",
      "|    ardeer|    3022|2/72 maxweld street|    -37.779824|    144.800113| 2.0|  2.0| 1.0|    townhouse| 213011328|Greater Melbourne|0.0491599999999999|       0.7966799999999999|              2.75698|2.4793000000000003|           3.46097|    22.44502|2023|       480.0|\n",
      "|    ardeer|    3022|2/72 maxweld street|    -37.779824|    144.800113| 2.0|  2.0| 1.0|    townhouse| 213011328|Greater Melbourne|0.0491599999999999|       0.7966799999999999|              2.75698|2.4793000000000003|           3.46097|    22.44502|2023|       380.0|\n",
      "|    ardeer|    3022|2/72 maxweld street|    -37.779824|    144.800113| 2.0|  2.0| 1.0|    townhouse| 213011328|Greater Melbourne|0.0491599999999999|       0.7966799999999999|              2.75698|2.4793000000000003|           3.46097|    22.44502|2021|       380.0|\n",
      "|    ardeer|    3022|2/72 maxweld street|    -37.779824|    144.800113| 2.0|  2.0| 1.0|    townhouse| 213011328|Greater Melbourne|0.0491599999999999|       0.7966799999999999|              2.75698|2.4793000000000003|           3.46097|    22.44502|2021|       380.0|\n",
      "|    ardeer|    3022|2/72 maxweld street|    -37.779824|    144.800113| 2.0|  2.0| 1.0|    townhouse| 213011328|Greater Melbourne|0.0491599999999999|       0.7966799999999999|              2.75698|2.4793000000000003|           3.46097|    22.44502|2019|       380.0|\n",
      "|    ardeer|    3022|2/72 maxweld street|    -37.779824|    144.800113| 2.0|  2.0| 1.0|    townhouse| 213011328|Greater Melbourne|0.0491599999999999|       0.7966799999999999|              2.75698|2.4793000000000003|           3.46097|    22.44502|2018|       365.0|\n",
      "|    ardeer|    3022|2/72 maxweld street|    -37.779824|    144.800113| 2.0|  2.0| 1.0|    townhouse| 213011328|Greater Melbourne|0.0491599999999999|       0.7966799999999999|              2.75698|2.4793000000000003|           3.46097|    22.44502|2018|       365.0|\n",
      "|black rock|    3193|  3, 342 beach road|  -37.98126001|  145.01895001| 3.0|  2.0| 2.0|         unit| 208011173|Greater Melbourne|            1.3383|                  0.25257|              3.97438|           0.81083|           2.40099|    23.56587|2023|      1000.0|\n",
      "|   macleod|    3085|  84 gresswell road|-37.7149392002|145.0632882002| 4.0|  2.0| 2.0|        house| 209021205|Greater Melbourne|3.5056100000000003|       0.8870399999999999|              2.96529|           2.82301|           3.06441|      16.835|2024|       950.0|\n",
      "|   macleod|    3085|  84 gresswell road|-37.7149392002|145.0632882002| 4.0|  2.0| 2.0|        house| 209021205|Greater Melbourne|3.5056100000000003|       0.8870399999999999|              2.96529|           2.82301|           3.06441|      16.835|2024|       950.0|\n",
      "|  mckinnon|    3204| 25a fitzroy street|-37.9143667002|145.0367358002| 3.0|  2.0| 2.0|        house| 208021174|Greater Melbourne|           1.16231|                  0.68035|              0.40346|0.7338899999999999|           0.86565|    17.07301|2024|       895.0|\n",
      "+----------+--------+-------------------+--------------+--------------+----+-----+----+-------------+----------+-----------------+------------------+-------------------------+---------------------+------------------+------------------+------------+----+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- suburb: string (nullable = true)\n",
      " |-- postcode: integer (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- beds: double (nullable = false)\n",
      " |-- baths: double (nullable = false)\n",
      " |-- cars: double (nullable = false)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- SA2_CODE21: string (nullable = true)\n",
      " |-- GCC_NAME21: string (nullable = true)\n",
      " |-- dist_to_education: string (nullable = true)\n",
      " |-- dist_to_parks_and_gardens: string (nullable = true)\n",
      " |-- dist_to_train_station: double (nullable = true)\n",
      " |-- dist_to_shopping: double (nullable = true)\n",
      " |-- dist_to_healthcare: double (nullable = true)\n",
      " |-- dist_to_city: double (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- weekly_price: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "JSON DECODE ERROR\n",
      "24/09/26 11:11:11 ERROR Utils: Aborting task                        (0 + 3) / 3]\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt/c/Users/pyrou/Documents/GitHub/project-2-group-real-estate-industry-project-31/notebooks/../scripts/preprocess_oldlistings.py\", line 286, in preprocess_dates\n",
      "    except ValueError:\n",
      "TypeError: 'float' object is not iterable\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "JSON DECODE ERROR\n",
      "24/09/26 11:11:11 ERROR FileFormatWriter: Job job_2024092611111019352022011490740_0007 aborted.\n",
      "24/09/26 11:11:11 ERROR Executor: Exception in task 2.0 in stage 7.0 (TID 10)\n",
      "org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/mnt/c/Users/pyrou/Documents/GitHub/project-2-group-real-estate-industry-project-31/data/raw/oldlisting/gm_oldlisting_final.csv.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt/c/Users/pyrou/Documents/GitHub/project-2-group-real-estate-industry-project-31/notebooks/../scripts/preprocess_oldlistings.py\", line 286, in preprocess_dates\n",
      "    except ValueError:\n",
      "TypeError: 'float' object is not iterable\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\t... 17 more\n",
      "24/09/26 11:11:11 WARN TaskSetManager: Lost task 2.0 in stage 7.0 (TID 10) (10.255.255.254 executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/mnt/c/Users/pyrou/Documents/GitHub/project-2-group-real-estate-industry-project-31/data/raw/oldlisting/gm_oldlisting_final.csv.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt/c/Users/pyrou/Documents/GitHub/project-2-group-real-estate-industry-project-31/notebooks/../scripts/preprocess_oldlistings.py\", line 286, in preprocess_dates\n",
      "    except ValueError:\n",
      "TypeError: 'float' object is not iterable\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\t... 17 more\n",
      "\n",
      "24/09/26 11:11:11 ERROR TaskSetManager: Task 2 in stage 7.0 failed 1 times; aborting job\n",
      "24/09/26 11:11:11 ERROR FileFormatWriter: Aborting job c9683a0d-07e1-4366-b9d6-b954026cb28e.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 7.0 failed 1 times, most recent failure: Lost task 2.0 in stage 7.0 (TID 10) (10.255.255.254 executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/mnt/c/Users/pyrou/Documents/GitHub/project-2-group-real-estate-industry-project-31/data/raw/oldlisting/gm_oldlisting_final.csv.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt/c/Users/pyrou/Documents/GitHub/project-2-group-real-estate-industry-project-31/notebooks/../scripts/preprocess_oldlistings.py\", line 286, in preprocess_dates\n",
      "    except ValueError:\n",
      "TypeError: 'float' object is not iterable\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\t... 17 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:390)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:418)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/mnt/c/Users/pyrou/Documents/GitHub/project-2-group-real-estate-industry-project-31/data/raw/oldlisting/gm_oldlisting_final.csv.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt/c/Users/pyrou/Documents/GitHub/project-2-group-real-estate-industry-project-31/notebooks/../scripts/preprocess_oldlistings.py\", line 286, in preprocess_dates\n",
      "    except ValueError:\n",
      "TypeError: 'float' object is not iterable\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\t... 17 more\n",
      "24/09/26 11:11:11 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/09/26 11:11:11 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/09/26 11:11:11 ERROR FileFormatWriter: Job job_2024092611111019352022011490740_0007 aborted.\n",
      "24/09/26 11:11:11 WARN TaskSetManager: Lost task 0.0 in stage 7.0 (TID 8) (10.255.255.254 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 7.0 failed 1 times, most recent failure: Lost task 2.0 in stage 7.0 (TID 10) (10.255.255.254 executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/mnt/c/Users/pyrou/Documents/GitHub/project-2-group-real-estate-industry-project-31/data/raw/oldlisting/gm_oldlisting_final.csv.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt/c/Users/pyrou/Documents/GitHub/project-2-group-real-estate-industry-project-31/notebooks/../scripts/preprocess_oldlistings.py\", line 286, in preprocess_dates\n",
      "    except ValueError:\n",
      "TypeError: 'float' object is not iterable\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\t... 17 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/09/26 11:11:11 ERROR FileFormatWriter: Job job_2024092611111019352022011490740_0007 aborted.\n",
      "24/09/26 11:11:11 WARN TaskSetManager: Lost task 1.0 in stage 7.0 (TID 9) (10.255.255.254 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 7.0 failed 1 times, most recent failure: Lost task 2.0 in stage 7.0 (TID 10) (10.255.255.254 executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/mnt/c/Users/pyrou/Documents/GitHub/project-2-group-real-estate-industry-project-31/data/raw/oldlisting/gm_oldlisting_final.csv.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt/c/Users/pyrou/Documents/GitHub/project-2-group-real-estate-industry-project-31/notebooks/../scripts/preprocess_oldlistings.py\", line 286, in preprocess_dates\n",
      "    except ValueError:\n",
      "TypeError: 'float' object is not iterable\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\t... 17 more\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o321.csv.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 7.0 failed 1 times, most recent failure: Lost task 2.0 in stage 7.0 (TID 10) (10.255.255.254 executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/mnt/c/Users/pyrou/Documents/GitHub/project-2-group-real-estate-industry-project-31/data/raw/oldlisting/gm_oldlisting_final.csv.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/c/Users/pyrou/Documents/GitHub/project-2-group-real-estate-industry-project-31/notebooks/../scripts/preprocess_oldlistings.py\", line 286, in preprocess_dates\n    except ValueError:\nTypeError: 'float' object is not iterable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n\t... 17 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:390)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:418)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/mnt/c/Users/pyrou/Documents/GitHub/project-2-group-real-estate-industry-project-31/data/raw/oldlisting/gm_oldlisting_final.csv.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/c/Users/pyrou/Documents/GitHub/project-2-group-real-estate-industry-project-31/notebooks/../scripts/preprocess_oldlistings.py\", line 286, in preprocess_dates\n    except ValueError:\nTypeError: 'float' object is not iterable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n\t... 17 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpreprocess_olist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/pyrou/Documents/GitHub/project-2-group-real-estate-industry-project-31/notebooks/../scripts/preprocess_oldlistings.py:51\u001b[0m, in \u001b[0;36mpreprocess_olist\u001b[0;34m(spark)\u001b[0m\n\u001b[1;32m     49\u001b[0m listings_df\u001b[38;5;241m.\u001b[39mprintSchema()\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m region \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgm_c+a_oldlisting.csv\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[43mlistings_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/raw/oldlisting/gm_oldlisting_final.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     listings_df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mrv_oldlisting_final.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/readwriter.py:1864\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m   1847\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m   1848\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1862\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[1;32m   1863\u001b[0m )\n\u001b[0;32m-> 1864\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o321.csv.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 7.0 failed 1 times, most recent failure: Lost task 2.0 in stage 7.0 (TID 10) (10.255.255.254 executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/mnt/c/Users/pyrou/Documents/GitHub/project-2-group-real-estate-industry-project-31/data/raw/oldlisting/gm_oldlisting_final.csv.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/c/Users/pyrou/Documents/GitHub/project-2-group-real-estate-industry-project-31/notebooks/../scripts/preprocess_oldlistings.py\", line 286, in preprocess_dates\n    except ValueError:\nTypeError: 'float' object is not iterable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n\t... 17 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:390)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:418)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/mnt/c/Users/pyrou/Documents/GitHub/project-2-group-real-estate-industry-project-31/data/raw/oldlisting/gm_oldlisting_final.csv.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/c/Users/pyrou/Documents/GitHub/project-2-group-real-estate-industry-project-31/notebooks/../scripts/preprocess_oldlistings.py\", line 286, in preprocess_dates\n    except ValueError:\nTypeError: 'float' object is not iterable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n\t... 17 more\n"
     ]
    }
   ],
   "source": [
    "preprocess_olist(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118038\n",
      "27989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "create_forecast_template(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----------------------------------------+--------------+--------------+----+-----+----+----------+------------------------------------------------------------------------------------------------------------------------------+---------+--------------+------------+\n",
      "|suburb    |postcode|address                                  |latitude      |longitude     |beds|baths|cars|house_type|dates                                                                                                                         |avg_price|classification|weekly_price|\n",
      "+----------+--------+-----------------------------------------+--------------+--------------+----+-----+----+----------+------------------------------------------------------------------------------------------------------------------------------+---------+--------------+------------+\n",
      "|Abbotsford|3067    |203/1 TURNER ST, ABBOTSFORD, ABBOTSFORD  |-37.7993152002|144.9975063002|2.0 |2.0  |1.0 |Unit/apmt |['September 2024']                                                                                                            |700.0    |week          |700.0       |\n",
      "|Abbotsford|3067    |117 TURNER STREET, ABBOTSFORD, ABBOTSFORD|-37.799524    |144.9997368   |3.0 |2.0  |1.0 |House     |['September 2024']                                                                                                            |1450.0   |week          |1450.0      |\n",
      "|Abbotsford|3067    |4/85 NICHOLSON STREET, ABBOTSFORD        |-37.8062248   |144.9966012   |3.0 |2.0  |1.0 |House     |['September 2024', 'April 2022', 'March 2022', 'April 2022', 'February 2021', 'January 2018', 'January 2018', 'December 2016']|800.0    |week          |800.0       |\n",
      "|Abbotsford|3067    |4/85 NICHOLSON STREET, ABBOTSFORD        |-37.8062248   |144.9966012   |3.0 |2.0  |1.0 |House     |['September 2024', 'April 2022', 'March 2022', 'April 2022', 'February 2021', 'January 2018', 'January 2018', 'December 2016']|800.0    |week          |800.0       |\n",
      "|Abbotsford|3067    |4/85 NICHOLSON STREET, ABBOTSFORD        |-37.8062248   |144.9966012   |3.0 |2.0  |1.0 |House     |['September 2024', 'April 2022', 'March 2022', 'April 2022', 'February 2021', 'January 2018', 'January 2018', 'December 2016']|800.0    |week          |800.0       |\n",
      "|Abbotsford|3067    |4/85 NICHOLSON STREET, ABBOTSFORD        |-37.8062248   |144.9966012   |3.0 |2.0  |1.0 |House     |['September 2024', 'April 2022', 'March 2022', 'April 2022', 'February 2021', 'January 2018', 'January 2018', 'December 2016']|790.0    |week          |790.0       |\n",
      "|Abbotsford|3067    |4/85 NICHOLSON STREET, ABBOTSFORD        |-37.8062248   |144.9966012   |3.0 |2.0  |1.0 |House     |['September 2024', 'April 2022', 'March 2022', 'April 2022', 'February 2021', 'January 2018', 'January 2018', 'December 2016']|800.0    |week          |800.0       |\n",
      "|Abbotsford|3067    |4/85 NICHOLSON STREET, ABBOTSFORD        |-37.8062248   |144.9966012   |3.0 |2.0  |1.0 |House     |['September 2024', 'April 2022', 'March 2022', 'April 2022', 'February 2021', 'January 2018', 'January 2018', 'December 2016']|800.0    |week          |800.0       |\n",
      "|Abbotsford|3067    |4/85 NICHOLSON STREET, ABBOTSFORD        |-37.8062248   |144.9966012   |3.0 |2.0  |1.0 |House     |['September 2024', 'April 2022', 'March 2022', 'April 2022', 'February 2021', 'January 2018', 'January 2018', 'December 2016']|800.0    |week          |800.0       |\n",
      "|Abbotsford|3067    |4/85 NICHOLSON STREET, ABBOTSFORD        |-37.8062248   |144.9966012   |3.0 |2.0  |1.0 |House     |['September 2024', 'April 2022', 'March 2022', 'April 2022', 'February 2021', 'January 2018', 'January 2018', 'December 2016']|NULL     |week          |NULL        |\n",
      "|Abbotsford|3067    |3/82 TRENERRY CRES, ABBOTSFORD           |-37.799923    |144.999835    |2.0 |2.0  |1.0 |Unit      |['September 2024', 'July 2023', 'June 2023', 'July 2022', 'February 2019', 'March 2019', 'November 2007']                     |840.0    |week          |840.0       |\n",
      "|Abbotsford|3067    |3/82 TRENERRY CRES, ABBOTSFORD           |-37.799923    |144.999835    |2.0 |2.0  |1.0 |Unit      |['September 2024', 'July 2023', 'June 2023', 'July 2022', 'February 2019', 'March 2019', 'November 2007']                     |900.0    |week          |900.0       |\n",
      "|Abbotsford|3067    |3/82 TRENERRY CRES, ABBOTSFORD           |-37.799923    |144.999835    |2.0 |2.0  |1.0 |Unit      |['September 2024', 'July 2023', 'June 2023', 'July 2022', 'February 2019', 'March 2019', 'November 2007']                     |850.0    |week          |850.0       |\n",
      "|Abbotsford|3067    |3/82 TRENERRY CRES, ABBOTSFORD           |-37.799923    |144.999835    |2.0 |2.0  |1.0 |Unit      |['September 2024', 'July 2023', 'June 2023', 'July 2022', 'February 2019', 'March 2019', 'November 2007']                     |840.0    |week          |840.0       |\n",
      "|Abbotsford|3067    |3/82 TRENERRY CRES, ABBOTSFORD           |-37.799923    |144.999835    |2.0 |2.0  |1.0 |Unit      |['September 2024', 'July 2023', 'June 2023', 'July 2022', 'February 2019', 'March 2019', 'November 2007']                     |830.0    |week          |830.0       |\n",
      "|Abbotsford|3067    |3/82 TRENERRY CRES, ABBOTSFORD           |-37.799923    |144.999835    |2.0 |2.0  |1.0 |Unit      |['September 2024', 'July 2023', 'June 2023', 'July 2022', 'February 2019', 'March 2019', 'November 2007']                     |850.0    |week          |850.0       |\n",
      "|Abbotsford|3067    |3/82 TRENERRY CRES, ABBOTSFORD           |-37.799923    |144.999835    |2.0 |2.0  |1.0 |Unit      |['September 2024', 'July 2023', 'June 2023', 'July 2022', 'February 2019', 'March 2019', 'November 2007']                     |400.0    |week          |400.0       |\n",
      "|Abbotsford|3067    |206/118 VERE STREET, ABBOTSFORD          |-37.80239001  |144.99335001  |2.0 |1.0  |1.0 |Unit      |['September 2024', 'June 2022', 'February 2022', 'February 2022', 'October 2021', 'November 2019', 'October 2019']            |520.0    |week          |520.0       |\n",
      "|Abbotsford|3067    |206/118 VERE STREET, ABBOTSFORD          |-37.80239001  |144.99335001  |2.0 |1.0  |1.0 |Unit      |['September 2024', 'June 2022', 'February 2022', 'February 2022', 'October 2021', 'November 2019', 'October 2019']            |500.0    |week          |500.0       |\n",
      "|Abbotsford|3067    |206/118 VERE STREET, ABBOTSFORD          |-37.80239001  |144.99335001  |2.0 |1.0  |1.0 |Unit      |['September 2024', 'June 2022', 'February 2022', 'February 2022', 'October 2021', 'November 2019', 'October 2019']            |520.0    |week          |520.0       |\n",
      "+----------+--------+-----------------------------------------+--------------+--------------+----+-----+----+----------+------------------------------------------------------------------------------------------------------------------------------+---------+--------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, from_json, col, regexp_replace, regexp_extract, split, when, expr, trim, lower\n",
    "from pyspark.sql.types import ArrayType, StringType, IntegerType\n",
    "\n",
    "df = sdf.withColumn(\"price_str\", regexp_replace(\"price_str\", \"'\", '\"'))\n",
    "\n",
    "# Define the schema of the array inside the string\n",
    "array_schema = ArrayType(StringType())\n",
    "\n",
    "# Convert the string to an actual array using from_json\n",
    "df = df.withColumn(\"price_str\", from_json(\"price_str\", array_schema))\n",
    "\n",
    "# Explode the 'costs' array to flatten it\n",
    "df_flattened = df.withColumn(\"ind_price_str\", explode(\"price_str\"))\n",
    "\n",
    "# Select distinct costs\n",
    "distinct_costs = df_flattened.select(\"ind_price_str\").distinct()\n",
    "\n",
    "# Define patterns\n",
    "range_pattern = r'(\\$\\d{1,3}(?:,\\d{3})*|\\d+)\\s*-\\s*(\\$\\d{1,3}(?:,\\d{3})*|\\d+)'  # To correctly capture ranges with or without commas and currency symbols\n",
    "# price_pattern = r'\\$?(\\d{1,3}(?:,\\d{3})*|\\d+)'  # To capture single prices\n",
    "price_pattern = r'\\$?(\\d+(?:,\\d{3})*|\\d+)'\n",
    "suffix_pattern = r'\\s+([a-zA-Z\\s]+)$'  # Capture suffixes that are words at the end of the string\n",
    "\n",
    "df_processed = df_flattened.withColumn(\"range\", regexp_extract(\"ind_price_str\", range_pattern, 0)) \\\n",
    "                 .withColumn(\"single_price\", regexp_extract(\"ind_price_str\", price_pattern, 0)) \\\n",
    "                 .withColumn(\"avg_price\", when(col(\"range\") != \"\",\n",
    "                                               (expr(\"cast(regexp_replace(split(range, '-')[0], '[\\$,]', '') as double)\") +\n",
    "                                                expr(\"cast(regexp_replace(split(range, '-')[1], '[\\$,]', '') as double)\")) / 2)\n",
    "                                          .otherwise(expr(\"cast(regexp_replace(single_price, '[\\$,]', '') as double)\"))) \\\n",
    "                 .withColumn(\"suffix\", trim(regexp_extract(\"ind_price_str\", suffix_pattern, 1)))\n",
    "\n",
    "# Define a classification of suffixes based on keywords\n",
    "df_classified = df_processed.withColumn(\"classification\", \n",
    "                             when(col(\"suffix\") == \"\", when(col(\"avg_price\") >= 50000, \"likely sale\").otherwise(\"week\"))\n",
    "                              .when(lower(col(\"suffix\")).rlike(\"(?<!million)week|pw|wk\"), \"week\")\n",
    "                              .when(lower(col(\"suffix\")).rlike(\"month|pcm\"), \"month\")\n",
    "                              .when(lower(col(\"suffix\")).rlike(\"annum|pa|annual\"), \"year\")\n",
    "                              .when(lower(col(\"suffix\")).rlike(\"season|seasonally\"), \"season\")\n",
    "                              .otherwise(\"other\"))\n",
    "\n",
    "df_adjusted = df_classified.withColumn(\"weekly_price\",\n",
    "                            when(col(\"classification\") == \"week\", col(\"avg_price\"))\n",
    "                            .when(col(\"classification\") == \"month\", col(\"avg_price\") / 4.3)\n",
    "                            .when(col(\"classification\") == \"year\", col(\"avg_price\") / 52)\n",
    "                            .when(col(\"classification\") == \"season\", col(\"avg_price\") / 13)\n",
    "                            )\n",
    "\n",
    "# Show results\n",
    "COLS_TO_DROP = [\"price_str\", \"ind_price_str\", \"range\", \"single_price\", \"suffix\"]\n",
    "df_adjusted = df_adjusted.drop(*COLS_TO_DROP)\n",
    "# df_flattened.filter(col(\"ind_price_str\").like(\"%Million High%\")).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "118038"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf = spark.read.parquet('../data/raw/oldlisting/oldlisting.parquet')\n",
    "ddf.count()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
